# -*- coding: utf-8 -*-
"""Copy of CNNproject .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tomr0t7JZ2UYD76ikKZWmORjW0_9CkIh
"""

# 1) Install the Kaggle API library
!pip install -q kaggle

# 2) Create the Kaggle credentials file (~/.kaggle/kaggle.json) automatically
import os

# Ensure the .kaggle directory exists
os.makedirs(os.path.expanduser("~/.kaggle"), exist_ok=True)

# Write your Kaggle username and API key into kaggle.json
with open(os.path.expanduser("~/.kaggle/kaggle.json"), "w") as f:
    f.write('{"username":"malathabdulghani","key":"de291308324d958049c6cfe2f122aa37"}')

# Restrict file permissions so only the owner can read/write
os.chmod(os.path.expanduser("~/.kaggle/kaggle.json"), 0o600)

# 3) Define dataset parameters
DATASET_PATH = "msambare/fer2013"   # Kaggle dataset identifier (username/dataset-name)
ZIP_NAME     = "fer2013.zip"        # Name of the downloaded zip file
DOWNLOAD_DIR = "/content/data"      # Directory where the dataset will be stored

# 4) Download the dataset from Kaggle
!kaggle datasets download -d {DATASET_PATH} -p {DOWNLOAD_DIR}

# 5) Unzip the downloaded dataset into the specified folder
!unzip -q {DOWNLOAD_DIR}/{ZIP_NAME} -d {DOWNLOAD_DIR}

# 6) Confirmation message
print(f" Dataset '{DATASET_PATH}' has been downloaded to '{DOWNLOAD_DIR}' and extracted successfully.")

print(" Dataset downloaded and extracted successfully.")

import os

# List the contents of the current working directory after extracting the dataset
for item in os.listdir():
    print(item)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Step 1: Initialize the image data generator with normalization (rescale pixel values to [0, 1])
datagen = ImageDataGenerator(rescale=1./255)

# Step 2: Load training data from the 'train' directory
train_generator = datagen.flow_from_directory(
    '/content/data/train',          # /content/data/train
    target_size=(48, 48),            # Resize all images to 48x48
    color_mode='grayscale',          # Use grayscale images (1 channel)
    batch_size=64,                   # Number of images per batch
    class_mode='categorical'         # Use categorical labels for multi-class classification
)

# Step 3: Load testing data from the 'test' directory
test_generator = datagen.flow_from_directory(
    '/content/data/test',            # /content/data/test
    target_size=(48, 48),
    color_mode='grayscale',
    batch_size=64,
    class_mode='categorical'
)

# Building a Convolutional Neural Network (CNN) model to classify emotions from grayscale images

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Initialize a sequential model to add layers in order
model = Sequential()

# First convolutional layer: 32 filters of size 3x3 with ReLU activation
# Input shape is (48, 48, 1) for grayscale images
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))

# MaxPooling reduces the spatial dimensions to lower computational cost
model.add(MaxPooling2D(pool_size=(2, 2)))

# Second convolutional layer: 64 filters for deeper feature extraction
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Third convolutional layer: 128 filters for more detailed feature learning
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# @title Default title text
model.summary()

from tensorflow.keras.layers import Flatten, Dense, Dropout

# Flatten before passing to fully connected layers
model.add(Flatten())

# Fully connected layer with 128 neurons
model.add(Dense(128, activation='relu'))

# Dropout to reduce overfitting
model.add(Dropout(0.5))

# Output layer: 7 classes (for emotions), using softmax for multi-class classification
model.add(Dense(7, activation='softmax'))

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

!pip install -q scikit-fuzzy

import numpy as np
import matplotlib.pyplot as plt
import skfuzzy as fuzz

# Step 1: Take a prediction from the model (example: 1 image from the test set)
x_sample, y_sample = next(test_generator)
prediction = model.predict(x_sample[:1])[0]  # Softmax output

# Extract the maximum probability as a representation of model confidence
confidence = np.max(prediction)

# Step 2: Define the universe of discourse for the confidence value (0 to 1)
x = np.arange(0, 1.01, 0.01)

# Step 3: Define triangular membership functions for fuzzy confidence levels
low = fuzz.trimf(x, [0.0, 0.0, 0.6])
medium = fuzz.trimf(x, [0.3, 0.5, 0.7])
high = fuzz.trimf(x, [0.6, 1.0, 1.0])

# Step 4: Calculate degree of membership for the current confidence value
low_conf = fuzz.interp_membership(x, low, confidence)
med_conf = fuzz.interp_membership(x, medium, confidence)
high_conf = fuzz.interp_membership(x, high, confidence)

# Display the fuzzy confidence values
print("Model's max confidence:", round(confidence, 3))
print("Degree of membership in LOW confidence:", round(low_conf, 3))
print("Degree of membership in MEDIUM confidence:", round(med_conf, 3))
print("Degree of membership in HIGH confidence:", round(high_conf, 3))

# Visualizing the fuzzy membership functions with the current confidence marked

plt.figure(figsize=(8, 5))
plt.plot(x, low, 'b', label='Low Confidence')
plt.plot(x, medium, 'g', label='Medium Confidence')
plt.plot(x, high, 'r', label='High Confidence')
plt.axvline(x=confidence, color='k', linestyle='--', label=f'Current Confidence = {round(confidence, 2)}')

plt.title('Fuzzy Membership Functions for Model Confidence')
plt.xlabel('Confidence Score')
plt.ylabel('Membership Degree')
plt.legend()
plt.grid(True)
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

# Build an enhanced CNN model with added regularization and normalization
model = Sequential()

# First convolutional block
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# Second convolutional block
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# Third convolutional block
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flattening and dense layers
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))

# Output layer with 7 classes (for FER2013)
model.add(Dense(7, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Return the model summary to review architecture
model.summary()

# Import necessary modules
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Step 1: Data preprocessing and augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    '/content/data/train',
    target_size=(48, 48),
    color_mode='grayscale',
    batch_size=64,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    '/content/data/test',
    target_size=(48, 48),
    color_mode='grayscale',
    batch_size=64,
    class_mode='categorical'
)

# Step 2: Build an enhanced CNN model
model = Sequential()

# First convolutional block
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# Second convolutional block
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# Third convolutional block
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten and Dense layers
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))  # <-- تم تقليلها من 0.5 إلى 0.3

# Output layer
model.add(Dense(7, activation='softmax'))

# Step 3: Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 4: Set callbacks
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=1e-6
)

# Step 5: Train the model
history = model.fit(
    train_generator,
    epochs=30,
    validation_data=test_generator,
    callbacks=[early_stop, reduce_lr]
)

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

model.save('emotion_model.h5')

# Import necessary libraries
# I used numpy for array operations, and seaborn/matplotlib for visualization.
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Evaluate the trained model on the test set
# I used the model.evaluate function to measure the final loss and accuracy on unseen data.
score = model.evaluate(test_generator, return_dict=True)
print(f"Test Accuracy: {score['accuracy']:.4f}")
print(f"Test Loss: {score['loss']:.4f}")

# Step 2: Generate predictions for the test data
# After evaluation, I generated predictions using model.predict.
# I applied np.argmax to get the class with the highest probability for each sample.
y_pred = model.predict(test_generator)
y_pred = np.argmax(y_pred, axis=1)

# Step 3: Extract the true labels from the test set
# Here, I extracted the actual labels to compare them with the predictions.
y_true = test_generator.classes

# Step 4: Plot the confusion matrix
# I created a confusion matrix using sklearn's confusion_matrix to analyze the classification performance.
confusion_mtx = confusion_matrix(y_true, y_pred)

# For better visualization, I plotted the confusion matrix using seaborn's heatmap.
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_mtx, annot=True, fmt='g', cmap='magma',
            xticklabels=list(test_generator.class_indices.keys()),
            yticklabels=list(test_generator.class_indices.keys()))
plt.xlabel('Prediction')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Emotion Classification')
plt.show()